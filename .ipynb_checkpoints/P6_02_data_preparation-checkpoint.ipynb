{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4413f1b5-4d8d-48a8-86fd-cc48f44113c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import io\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# Import image processing/reading libraries\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform\n",
    "import albumentations as A\n",
    "\n",
    "\n",
    "\n",
    "# CNN libraries\n",
    "# example of tending the vgg16 model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import os\n",
    "from keras.layers import Flatten\n",
    "\n",
    "# Import NLP libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "# Other libraries\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4548295-0be1-43cf-9454-4838cb0e20c2",
   "metadata": {},
   "source": [
    "## BOW - TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "572df468-14df-4760-9897-f5a0093b968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "description=pd.read_csv(\"../data/Dataset+projet+prétraitement+textes+images/Flipkart/flipkart_com-ecommerce_sample_1050.csv\")\n",
    "\n",
    "## Preprocessing du text avec transformation en minuscule, eliminiation de stopwords\n",
    "def text_processing(x):\n",
    "    x=x.lower()\n",
    "    pattern = r'[0-9]'\n",
    "    x = re.sub(pattern, '', x)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(x)\n",
    "    word_tokens = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "            \n",
    "    return \" \".join(filtered_sentence)\n",
    "\n",
    "description[\"processed_description\"]=description[\"description\"].apply(lambda x: text_processing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "526ccef5-e0d5-4ce9-8b1f-3921bad83bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application de TF-IDF\n",
    "transformer = TfidfVectorizer(max_df=0.5,min_df=0.001)\n",
    "vectors = transformer.fit_transform(description[\"processed_description\"])\n",
    "feature_names = transformer.get_feature_names()\n",
    "dense = vectors.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbfd3a84-6161-4be5-bfa9-f13367bae9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du fichier vectoriser sous forme serialisee\n",
    "with open(\"tfidf_vectorizer.pkl\",\"wb\") as f:\n",
    "    pickle.dump(df,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d934c324-4761-4c97-be2b-a921fc716829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aapno</th>\n",
       "      <th>able</th>\n",
       "      <th>abode</th>\n",
       "      <th>absorbency</th>\n",
       "      <th>absorbent</th>\n",
       "      <th>abstract</th>\n",
       "      <th>abstracts</th>\n",
       "      <th>ac</th>\n",
       "      <th>accent</th>\n",
       "      <th>...</th>\n",
       "      <th>ym</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>youthful</th>\n",
       "      <th>yuva</th>\n",
       "      <th>zero</th>\n",
       "      <th>zipper</th>\n",
       "      <th>zone</th>\n",
       "      <th>zyxel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.183971</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1046</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1047</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1050 rows × 2304 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aa  aapno  able  abode  absorbency  absorbent  abstract  abstracts  \\\n",
       "0     0.0    0.0   0.0    0.0    0.000000        0.0  0.183971        0.0   \n",
       "1     0.0    0.0   0.0    0.0    0.000000        0.0  0.000000        0.0   \n",
       "2     0.0    0.0   0.0    0.0    0.076418        0.0  0.000000        0.0   \n",
       "3     0.0    0.0   0.0    0.0    0.000000        0.0  0.000000        0.0   \n",
       "4     0.0    0.0   0.0    0.0    0.000000        0.0  0.000000        0.0   \n",
       "...   ...    ...   ...    ...         ...        ...       ...        ...   \n",
       "1045  0.0    0.0   0.0    0.0    0.000000        0.0  0.000000        0.0   \n",
       "1046  0.0    0.0   0.0    0.0    0.000000        0.0  0.000000        0.0   \n",
       "1047  0.0    0.0   0.0    0.0    0.000000        0.0  0.000000        0.0   \n",
       "1048  0.0    0.0   0.0    0.0    0.000000        0.0  0.000000        0.0   \n",
       "1049  0.0    0.0   0.0    0.0    0.000000        0.0  0.000000        0.0   \n",
       "\n",
       "       ac  accent  ...   ym  york  young  youth  youthful  yuva  zero  zipper  \\\n",
       "0     0.0     0.0  ...  0.0   0.0    0.0    0.0       0.0   0.0   0.0     0.0   \n",
       "1     0.0     0.0  ...  0.0   0.0    0.0    0.0       0.0   0.0   0.0     0.0   \n",
       "2     0.0     0.0  ...  0.0   0.0    0.0    0.0       0.0   0.0   0.0     0.0   \n",
       "3     0.0     0.0  ...  0.0   0.0    0.0    0.0       0.0   0.0   0.0     0.0   \n",
       "4     0.0     0.0  ...  0.0   0.0    0.0    0.0       0.0   0.0   0.0     0.0   \n",
       "...   ...     ...  ...  ...   ...    ...    ...       ...   ...   ...     ...   \n",
       "1045  0.0     0.0  ...  0.0   0.0    0.0    0.0       0.0   0.0   0.0     0.0   \n",
       "1046  0.0     0.0  ...  0.0   0.0    0.0    0.0       0.0   0.0   0.0     0.0   \n",
       "1047  0.0     0.0  ...  0.0   0.0    0.0    0.0       0.0   0.0   0.0     0.0   \n",
       "1048  0.0     0.0  ...  0.0   0.0    0.0    0.0       0.0   0.0   0.0     0.0   \n",
       "1049  0.0     0.0  ...  0.0   0.0    0.0    0.0       0.0   0.0   0.0     0.0   \n",
       "\n",
       "      zone  zyxel  \n",
       "0      0.0    0.0  \n",
       "1      0.0    0.0  \n",
       "2      0.0    0.0  \n",
       "3      0.0    0.0  \n",
       "4      0.0    0.0  \n",
       "...    ...    ...  \n",
       "1045   0.0    0.0  \n",
       "1046   0.0    0.0  \n",
       "1047   0.0    0.0  \n",
       "1048   0.0    0.0  \n",
       "1049   0.0    0.0  \n",
       "\n",
       "[1050 rows x 2304 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27e12cd-5478-4882-981f-0b90099d7a8f",
   "metadata": {},
   "source": [
    "## Sentence vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "055fa13d-93a4-4f50-9fa9-4d60fad139ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b40a7211a5429ca99ffaa7de405aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2820f46e0cf844f2b1d4d74d88288eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/10.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77eb020ea0144c6fa5eb625c45d0bb28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec83f251966454d9eca677f644a2e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c8c70b781264e10a36b6582320ad1d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acc62b36802d477dab59aecda63159e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e319963b9bf4e058ceb4c892483e522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b287e1fea8ef4e07affa13d123224953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b207b6105e8447e5987d711d0aacd5d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a971c55cda8e4a1f99cbdbe120aeb57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec191a537ff740c3bdd8a99a4e44c9ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4991226b31f049be967304c7cffed6e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a678d68d274acfbfb9006a3076bb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b516b867bffe4b808fc34662044cf35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vectoriser avec un modele BERT\n",
    "description=pd.read_csv(\"../data/Dataset+projet+prétraitement+textes+images/Flipkart/flipkart_com-ecommerce_sample_1050.csv\")\n",
    "vectorizer = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "test=description[\"description\"].iloc[0]\n",
    "sentence_embeddings = vectorizer.encode(test)\n",
    "sentence_embeddings = sentence_embeddings.reshape(1,sentence_embeddings.shape[0])\n",
    "vectorized_description=[]\n",
    "for a in description[\"description\"]:\n",
    "    sentence_embeddings = vectorizer.encode(a)\n",
    "    sentence_embeddings = sentence_embeddings.reshape(1,sentence_embeddings.shape[0])\n",
    "    vectorized_description.append(sentence_embeddings.reshape(384))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a4a36-588a-4b91-98ae-ca066eb95aa9",
   "metadata": {},
   "source": [
    "## CNN Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d6611a00-f7ae-4ff7-8977-75ca06d27526",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "\n",
    "# Creation du modele  cnn VGG16 en modifiant l'input\n",
    "model = VGG16(weights='imagenet', include_top=False,input_shape=(258,477,3))\n",
    "flat1 = Flatten()(model.layers[-1].output)\n",
    "output = Dense(500, activation='softmax')(flat1)\n",
    "model=Model(inputs=model.inputs,outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4e90d37f-9780-48cc-9bae-6494bb4fd8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=cv2.imread(\"../data/Dataset+projet+prétraitement+textes+images/Flipkart/Images/7b72c92c2f6c40268628ec5f14c6d590.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "871cfece-7b34-4131-a433-eb2270bcac76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(258, 477, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format de l'image entrante\n",
    "scale_percent = 20 # percent of original size\n",
    "width = int(img.shape[1] * scale_percent / 100)\n",
    "height = int(img.shape[0] * scale_percent / 100)\n",
    "dim = (width, height)\n",
    "  \n",
    "# resize image\n",
    "resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c18ce976-d0b4-4560-93fd-3151e91178ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049\n"
     ]
    }
   ],
   "source": [
    "img=cv2.imread(\"../data/Dataset+projet+prétraitement+textes+images/Flipkart/Images/7b72c92c2f6c40268628ec5f14c6d590.jpg\")\n",
    "scale_percent = 20 # percent of original size\n",
    "width = int(img.shape[1] * scale_percent / 100)\n",
    "height = int(img.shape[0] * scale_percent / 100)\n",
    "dim = (width, height)\n",
    "cnn_features1=[]\n",
    "i=0\n",
    "# Extraction des features des images originales\n",
    "for a in description[\"image\"]:\n",
    "    print(i)\n",
    "    \n",
    "    img = cv2.imread(\"../data/Dataset+projet+prétraitement+textes+images/Flipkart/Images/\"+a)\n",
    "    resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    x = image.img_to_array(resized)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = model.predict(x)\n",
    "    cnn_features1.append(features.reshape(500))\n",
    "    i=i+1\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "features1=pd.concat([pd.DataFrame(cnn_features1),pd.DataFrame(vectorized_description)],axis=1,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b241ca7a-a2d1-4c89-8f63-b94e72ec1d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049\n"
     ]
    }
   ],
   "source": [
    "cnn_features2=[]\n",
    "i=0\n",
    "# Extraction des features des images data1\n",
    "for a in description[\"image\"]:\n",
    "    print(i)\n",
    "    img = cv2.imread(\"../data/data_pretraitement/data1/\"+a)\n",
    "    resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    x = image.img_to_array(resized)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = model.predict(x)\n",
    "    cnn_features2.append(features.reshape(500))\n",
    "    i=i+1\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "features2=pd.concat([pd.DataFrame(cnn_features2),pd.DataFrame(vectorized_description)],axis=1,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7eb70caa-31d0-42a2-a8e9-4ded7baee2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049\n"
     ]
    }
   ],
   "source": [
    "cnn_features3=[]\n",
    "i=0\n",
    "# Extraction des features des images data2\n",
    "for a in description[\"image\"]:\n",
    "    print(i)\n",
    "    img = cv2.imread(\"../data/data_pretraitement/data2/\"+a)\n",
    "    resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    x = image.img_to_array(resized)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = model.predict(x)\n",
    "    cnn_features3.append(features.reshape(500))\n",
    "    i=i+1\n",
    "    clear_output(wait=True)\n",
    "\n",
    "\n",
    "features3=pd.concat([pd.DataFrame(cnn_features3),pd.DataFrame(vectorized_description)],axis=1,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe2acf16-a982-4b7c-80e7-136ecab41963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049\n"
     ]
    }
   ],
   "source": [
    "cnn_features4=[]\n",
    "i=0\n",
    "# Extraction des features des images data3\n",
    "for a in description[\"image\"]:\n",
    "    print(i)\n",
    "    img = cv2.imread(\"../data/data_pretraitement/data3/\"+a)\n",
    "    resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    x = image.img_to_array(resized)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = model.predict(x)\n",
    "    cnn_features4.append(features.reshape(500))\n",
    "    i=i+1\n",
    "    clear_output(wait=True)\n",
    "\n",
    "\n",
    "features4=pd.concat([pd.DataFrame(cnn_features4),pd.DataFrame(vectorized_description)],axis=1,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b9345ed6-d1cf-4c7a-8298-1c4d09e25c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des features sous forme serialisee \n",
    "with open(\"vectorized_cnn.pkl\",\"wb\") as f:\n",
    "    pickle.dump([features1,features2,features3,features4],f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ef7116-b4a4-4f96-a6cc-22af39261892",
   "metadata": {},
   "source": [
    "## SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca187db9-623f-4991-ab9f-f804ed796060",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=cv2.imread(\"../data/Dataset+projet+prétraitement+textes+images/Flipkart/Images/7b72c92c2f6c40268628ec5f14c6d590.jpg\")\n",
    "scale_percent = 20 # percent of original size\n",
    "width = int(img.shape[1] * scale_percent / 100)\n",
    "height = int(img.shape[0] * scale_percent / 100)\n",
    "dim = (width, height)\n",
    "resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "gray_scale = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63c69100-ebb9-454f-907b-a303cdf5bcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction extrayant un dico de features par image\n",
    "def step1(description,path):\n",
    "    #sift_features=[]\n",
    "    i=0\n",
    "    dico=[]\n",
    "    for a in description[\"image\"]:\n",
    "        print(i)\n",
    "        img = cv2.imread(path+a)\n",
    "        width=img.shape[1]\n",
    "        height=img.shape[0]\n",
    "        if width > 500 or height > 500:\n",
    "            scale_percent = 20 # percent of original size\n",
    "            width = int(img.shape[1] * scale_percent / 100)\n",
    "            height = int(img.shape[0] * scale_percent / 100)\n",
    "            dim = (width, height)\n",
    "            resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "        else :\n",
    "            resized = img\n",
    "        #gray_scale = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "        sift = cv2.xfeatures2d.SIFT_create()\n",
    "        kp, des = sift.detectAndCompute(resized, None)\n",
    "        #sift_features.append(des)\n",
    "        if des is not None:\n",
    "            for d in des:\n",
    "                dico.append(d)\n",
    "        i=i+1\n",
    "        clear_output(wait=True)\n",
    "    return dico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b410d33-bb2e-4b99-a42f-493aba940979",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MiniBatchKMeans' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7a96f8386f97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m620\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMiniBatchKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdico\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'MiniBatchKMeans' is not defined"
     ]
    }
   ],
   "source": [
    "# Code de la fonction step1 sans fonction\n",
    "sift_features1=[]\n",
    "i=0\n",
    "dico=[]\n",
    "for a in description[\"image\"]:\n",
    "    print(i)\n",
    "    \n",
    "    img = cv2.imread(\"../data/Dataset+projet+prétraitement+textes+images/Flipkart/Images/\"+a)\n",
    "    width=img.shape[1]\n",
    "    height=img.shape[0]\n",
    "    if width > 500 or height > 500:\n",
    "        scale_percent = 20 # percent of original size\n",
    "        width = int(img.shape[1] * scale_percent / 100)\n",
    "        height = int(img.shape[0] * scale_percent / 100)\n",
    "        dim = (width, height)\n",
    "        resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    else :\n",
    "        resized = img\n",
    "    #gray_scale = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    kp, des = sift.detectAndCompute(resized, None)\n",
    "    sift_features1.append(des)\n",
    "    if des is not None:\n",
    "        for d in des:\n",
    "            dico.append(d)\n",
    "    i=i+1\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db9e7f81-6352-4675-9caa-09f6c1682b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction creant et fittant un algorithme kmeans sur un dictionnaire\n",
    "def step2(k,batch,dico):\n",
    "    kmeans = MiniBatchKMeans(n_clusters=k, batch_size=batch).fit(dico)\n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98ccd9d8-451f-4543-8f92-b85653e6aca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction creant une liste de features\n",
    "def step3(description,path,kmeans):\n",
    "    histo_list = []\n",
    "    i=0\n",
    "    for a in description[\"image\"]:\n",
    "        print(i)\n",
    "\n",
    "        img = cv2.imread(path+a)\n",
    "        print(path+a)\n",
    "        width=img.shape[1]\n",
    "        height=img.shape[0]\n",
    "        if width > 500 or height > 500:\n",
    "            scale_percent = 20 # percent of original size\n",
    "            width = int(img.shape[1] * scale_percent / 100)\n",
    "            height = int(img.shape[0] * scale_percent / 100)\n",
    "            dim = (width, height)\n",
    "            resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "        else :\n",
    "            resized = img\n",
    "        sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "        kp, des = sift.detectAndCompute(resized, None)  \n",
    "        histo = np.zeros(620)\n",
    "        nkp = np.size(kp)\n",
    "\n",
    "        if des is not None:\n",
    "            for d in des:\n",
    "                idx = kmeans.predict([d])\n",
    "                histo[idx] += 1/nkp \n",
    "        i=i+1\n",
    "        clear_output(wait=True)\n",
    "        histo_list.append(histo)\n",
    "    return histo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3df8e0b9-1e69-480c-9287-5b59792cfa08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049\n"
     ]
    }
   ],
   "source": [
    "# Code de la fonction step3 sans fonction\n",
    "histo_list = []\n",
    "i=0\n",
    "for a in description[\"image\"]:\n",
    "    print(i)\n",
    "    \n",
    "    img = cv2.imread(\"../data/Dataset+projet+prétraitement+textes+images/Flipkart/Images/\"+a)\n",
    "    width=img.shape[1]\n",
    "    height=img.shape[0]\n",
    "    if width > 500 or height > 500:\n",
    "        scale_percent = 20 # percent of original size\n",
    "        width = int(img.shape[1] * scale_percent / 100)\n",
    "        height = int(img.shape[0] * scale_percent / 100)\n",
    "        dim = (width, height)\n",
    "        resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    else :\n",
    "        resized = img\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    \n",
    "    kp, des = sift.detectAndCompute(resized, None)  \n",
    "    histo = np.zeros(620)\n",
    "    nkp = np.size(kp)\n",
    "\n",
    "    if des is not None:\n",
    "        for d in des:\n",
    "            idx = kmeans.predict([d])\n",
    "            histo[idx] += 1/nkp \n",
    "    i=i+1\n",
    "    clear_output(wait=True)\n",
    "    histo_list.append(histo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c6d4ea3-746d-4a74-973c-4f6fa41f9ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049\n",
      "../data/Dataset+projet+prétraitement+textes+images/Flipkart/Images/f2f027ad6a6df617c9f125173da71e44.jpg\n"
     ]
    }
   ],
   "source": [
    "# Creation des features visuelles et textuelles pour les images originales\n",
    "dico = step1(description,\"../data/Dataset+projet+prétraitement+textes+images/Flipkart/Images/\")\n",
    "kmeans = step2(620,100,dico)\n",
    "histo_list = step3(description,\"../data/Dataset+projet+prétraitement+textes+images/Flipkart/Images/\",kmeans)\n",
    "siftfeatures1=pd.concat([pd.DataFrame(histo_list),df],ignore_index=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ea54e98-cec5-4b0c-82f4-35209e911217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049\n",
      "../data/data_pretraitement/data1/f2f027ad6a6df617c9f125173da71e44.jpg\n"
     ]
    }
   ],
   "source": [
    "# Creation des features visuelles et textuelles pour les images data1\n",
    "dico = step1(description,\"../data/data_pretraitement/data1/\")\n",
    "kmeans = step2(620,100,dico)\n",
    "histo_list = step3(description,\"../data/data_pretraitement/data1/\",kmeans)\n",
    "siftfeatures2=pd.concat([pd.DataFrame(histo_list),df],ignore_index=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d224444-172f-47e0-8ea1-000a578e4e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049\n",
      "../data/data_pretraitement/data2/f2f027ad6a6df617c9f125173da71e44.jpg\n"
     ]
    }
   ],
   "source": [
    "# Creation des features visuelles et textuelles pour les images data2\n",
    "dico = step1(description,\"../data/data_pretraitement/data2/\")\n",
    "kmeans = step2(620,100,dico)\n",
    "histo_list = step3(description,\"../data/data_pretraitement/data2/\",kmeans)\n",
    "siftfeatures3=pd.concat([pd.DataFrame(histo_list),df],ignore_index=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7428d273-665d-4bd5-9781-f8d61c65b58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1049\n",
      "../data/data_pretraitement/data3/f2f027ad6a6df617c9f125173da71e44.jpg\n"
     ]
    }
   ],
   "source": [
    "# Creation des features visuelles et textuelles pour les images data3\n",
    "dico = step1(description,\"../data/data_pretraitement/data3/\")\n",
    "kmeans = step2(620,100,dico)\n",
    "histo_list = step3(description,\"../data/data_pretraitement/data3/\",kmeans)\n",
    "siftfeatures4=pd.concat([pd.DataFrame(histo_list),df],ignore_index=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e1ca496b-1749-41be-a21b-28b0dbc5b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du fichier sous format serialise\n",
    "with open(\"vectorized_sift.pkl\",\"wb\") as f:\n",
    "    pickle.dump([siftfeatures1,siftfeatures2,siftfeatures3,siftfeatures4],f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
